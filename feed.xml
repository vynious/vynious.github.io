<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vynious.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vynious.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-18T14:45:39+00:00</updated><id>https://vynious.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Maximizing Client Throughput — Async vs Threads, Adaptive Rate Limiting, and Queue Resilience</title><link href="https://vynious.github.io/blog/2025/optimise-throughput/" rel="alternate" type="text/html" title="Maximizing Client Throughput — Async vs Threads, Adaptive Rate Limiting, and Queue Resilience"/><published>2025-09-18T12:00:00+00:00</published><updated>2025-09-18T12:00:00+00:00</updated><id>https://vynious.github.io/blog/2025/optimise-throughput</id><content type="html" xml:base="https://vynious.github.io/blog/2025/optimise-throughput/"><![CDATA[<blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>Prefer a <strong>single authoritative limiter</strong> (circular buffer) and remove redundant sleeps. Add a <strong>small adaptive buffer</strong> for server-side timestamp skew to avoid 429s.</p> </blockquote> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>If <strong>request generation &gt; dequeue rate</strong>, your main queue will bloat and TTLs will expire. Add <strong>backpressure</strong> or increase <strong>consumer parallelism</strong>.</p> </blockquote> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>Relying on a <strong>fixed latency buffer</strong> (e.g., always 50 ms) can silently cut your throughput on good networks <strong>or</strong> still 429 on bad ones. Use <strong>adaptive buffering</strong>.</p> </blockquote> <hr/> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><strong><a href="#getting-started">Getting Started</a></strong></li> <li><strong><a href="#folder-structure">Folder Structure</a></strong></li> <li><strong><a href="#introduction">Introduction</a></strong></li> <li><strong><a href="#benchmarking-system">Benchmarking System</a></strong> <ul> <li><a href="#key-metrics-tracked">Key Metrics Tracked</a></li> <li><a href="#how-the-benchmarking-system-works">How the Benchmarking System Works</a></li> </ul> </li> <li><strong><a href="#enhancing-the-rate-limiter">Enhancing the Rate Limiter</a></strong> <ul> <li><a href="#original-implementation">Original Implementation</a></li> <li><a href="#identified-issues">Identified Issues</a></li> <li><a href="#proposed-solution">Proposed Solution</a></li> <li><a href="#observation-performance-improvement">Observation: Performance Improvement</a></li> <li><a href="#potential-issue-429-errors-due-to-latency">Potential Issue: 429 Errors Due to Latency</a></li> <li><a href="#improved-version-adaptive-buffering">Improved Version: Adaptive Buffering</a></li> </ul> </li> <li><strong><a href="#improving-the-queue-system">Improving the Queue System</a></strong> <ul> <li><a href="#issue">Issue</a></li> <li><a href="#solution-queue-manager-with-dead-letter-queue-dlq">Solution: Queue Manager with Dead Letter Queue (DLQ)</a></li> <li><a href="#lifecycle-with-queue-manager">Lifecycle with Queue Manager</a></li> <li><a href="#monitoring-the-queue">Monitoring the Queue</a></li> </ul> </li> <li><strong><a href="#addressing-queue-bloating">Addressing Queue Bloating</a></strong></li> <li><strong><a href="#exploring-multithreading">Exploring Multithreading</a></strong> <ul> <li><a href="#rationale">Rationale</a></li> <li><a href="#changes-to-the-current-code">Changes to the Current Code</a></li> </ul> </li> <li><strong><a href="#comparison-between-asynchronous-and-multithreading-client">Comparison Between Asynchronous and Multithreading Client</a></strong> <ul> <li><a href="#baseline-comparison">Baseline Comparison</a></li> <li><a href="#observation">Observation</a></li> <li><a href="#summary">Summary</a></li> </ul> </li> <li><strong><a href="#overview-and-modifications-summary">Overview and Modifications Summary</a></strong> <ul> <li><a href="#modifying-the-original-client">Modifying the Original Client</a></li> </ul> </li> <li><strong><a href="#final-thoughts">Final Thoughts</a></strong></li> </ol> <hr/> <h2 id="getting-started">Getting Started</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># run the server</span>
python3 original_server.py

<span class="c"># run async client</span>
python3 async/client.py

<span class="c"># run multithreaded client</span>
python3 thread/client.py

<span class="c"># run with memory profiling</span>
mprof run python3 &lt;MODEL&gt;/client.py
mprof plot
</code></pre></div></div> <hr/> <h2 id="folder-structure">Folder Structure</h2> <ul> <li><code class="language-plaintext highlighter-rouge">async/</code> — asynchronous client (rate limiter, benchmark, queue manager)</li> <li><code class="language-plaintext highlighter-rouge">thread/</code> — multithreaded client (thread-safe queue manager and limiter)</li> <li><code class="language-plaintext highlighter-rouge">original_client.py</code> — baseline client</li> <li><code class="language-plaintext highlighter-rouge">original_server.py</code> — baseline server</li> </ul> <hr/> <h2 id="introduction">Introduction</h2> <p>This post reviews and optimizes a Python client designed to <strong>maximize throughput</strong> under <strong>server-enforced rate limits</strong> using multiple API keys. We identify bottlenecks in the original implementation, introduce an <strong>adaptive rate limiter</strong>, add <strong>queue resilience</strong> with a <strong>DLQ</strong>, and compare <strong>async vs threading</strong> under realistic load.</p> <hr/> <h2 id="benchmarking-system">Benchmarking System</h2> <blockquote> <h5 id="tip-1">TIP</h5> <p class="block-tip">Benchmarks live in <code class="language-plaintext highlighter-rouge">async/benchmark.py</code> and <code class="language-plaintext highlighter-rouge">thread/benchmark.py</code>. Keep runner overhead minimal and print <strong>aggregate stats</strong> periodically.</p> </blockquote> <h3 id="key-metrics-tracked">Key Metrics Tracked</h3> <ol> <li><strong>Total Successful Requests</strong> — stability and efficiency under load</li> <li><strong>Total Failed Requests</strong> — timeouts / 429s / network issues</li> <li><strong>Average Latency (ms)</strong> — user-perceived performance</li> <li><strong>Throughput (TPS)</strong> — sustainable capacity from start of run</li> </ol> <h3 id="how-the-benchmarking-system-works">How the Benchmarking System Works</h3> <ul> <li>Records success/failure and latencies</li> <li>Computes moving <strong>average latency</strong></li> <li>Derives <strong>throughput</strong> since benchmark start</li> <li>Prints metrics at fixed intervals for <strong>real-time feedback</strong></li> </ul> <hr/> <h2 id="enhancing-the-rate-limiter">Enhancing the Rate Limiter</h2> <h3 id="original-implementation">Original Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RateLimiter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">per_second_rate</span><span class="p">,</span> <span class="n">min_duration_ms_between_requests</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span> <span class="o">=</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span> <span class="o">=</span> <span class="n">min_duration_ms_between_requests</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@contextlib.asynccontextmanager</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">acquire</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timeout_ms</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">enter_ms</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">now</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">enter_ms</span> <span class="o">&gt;</span> <span class="n">timeout_ms</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">RateLimiterTimeout</span><span class="p">()</span>

            <span class="c1"># Fixed Interval Check
</span>            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Circular Buffer Check
</span>            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="k">break</span>

        <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">now</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span>
        <span class="k">yield</span> <span class="n">self</span>
</code></pre></div></div> <h3 id="identified-issues">Identified Issues</h3> <p>Two separate sleeps attempt to regulate rate:</p> <ul> <li><strong>Fixed interval</strong> between consecutive requests</li> <li><strong>Circular buffer</strong> enforcing N/second</li> </ul> <p>They are <strong>redundant</strong> and cause extra context switches.</p> <h3 id="proposed-solution">Proposed Solution</h3> <p>Remove the <strong>fixed interval</strong>; rely solely on the <strong>circular buffer</strong> (N requests per any sliding 1 s window). This reduces yield/scheduling overhead and still handles <strong>bursty</strong> and <strong>constant-rate</strong> traffic.</p> <h3 id="observation-performance-improvement">Observation: Performance Improvement</h3> <p>Throughput improved from <strong>~74 TPS → ~85 TPS</strong> after removing the fixed interval check, primarily by cutting <strong>event-loop churn</strong>.</p> <h3 id="potential-issue-429-errors-due-to-latency">Potential Issue: 429 Errors Due to Latency</h3> <p>Even with a correct client window, <strong>server-side timestamps</strong> (affected by variable network latency) can observe <strong>Δt &lt; 1000 ms</strong> between the 1st and Nth requests → <strong>429</strong>.</p> <blockquote> <h5 id="warning-1">WARNING</h5> <p class="block-warning">Timestamp skew of just a few milliseconds between client/server can flip a pass into a fail. Add a <strong>latency headroom</strong>.</p> </blockquote> <h3 id="improved-version-adaptive-buffering">Improved Version: Adaptive Buffering</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RateLimiter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">per_second_rate</span><span class="p">,</span> <span class="n">min_duration_ms_between_requests</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span> <span class="o">=</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span> <span class="o">=</span> <span class="n">min_duration_ms_between_requests</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span> <span class="o">=</span> <span class="mi">40</span>     <span class="c1"># ms
</span>        <span class="n">self</span><span class="p">.</span><span class="n">__min_buffer</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># ms
</span>        <span class="n">self</span><span class="p">.</span><span class="n">__max_buffer</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># ms
</span>
    <span class="k">def</span> <span class="nf">update_buffer</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">:</span>
            <span class="n">avg_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__max_buffer</span><span class="p">,</span>
                                <span class="nf">max</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__min_buffer</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">avg_latency</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">record_latency</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latency_ms</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency_ms</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">update_buffer</span><span class="p">()</span>

    <span class="nd">@contextlib.asynccontextmanager</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">acquire</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timeout_ms</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">enter_ms</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="c1"># headroom: circular window (1000ms) + adaptive buffer
</span>        <span class="n">headroom</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">now</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">enter_ms</span> <span class="o">&gt;</span> <span class="n">timeout_ms</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">RateLimiterTimeout</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">headroom</span><span class="p">:</span>
                <span class="n">sleep_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">headroom</span> <span class="o">-</span> <span class="p">(</span><span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">1000</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">break</span>

        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span>
        <span class="k">yield</span> <span class="n">self</span>
</code></pre></div></div> <blockquote> <h5 id="tip-2">TIP</h5> <p class="block-tip">Still seeing sporadic 429s? Add <strong>exponential backoff with jitter</strong> on retries and clamp max in-flight requests per key.</p> </blockquote> <hr/> <h2 id="improving-the-queue-system">Improving the Queue System</h2> <h3 id="issue">Issue</h3> <p>Request generation can <strong>outpace</strong> consumption: TTLs expire, wasting work.</p> <h3 id="solution-queue-manager-with-dead-letter-queue-dlq">Solution: Queue Manager with Dead Letter Queue (DLQ)</h3> <ul> <li><strong>Main Queue</strong> — normal flow</li> <li><strong>DLQ</strong> — failures/timeouts for <strong>retry</strong> with capped attempts</li> <li><strong>Graveyard</strong> — IDs exceeding max retries for later analysis</li> </ul> <p><strong>Retry Prioritization:</strong> Insert short <strong>cooldowns</strong> in producers so DLQ items can be re-slotted quickly (lightweight alternative to a strict priority queue).</p> <h3 id="lifecycle-with-queue-manager">Lifecycle with Queue Manager</h3> <p><img src="./img/uml_seq_diagram.png" alt="New Sequence with Queue Manager"/></p> <h3 id="monitoring-the-queue">Monitoring the Queue</h3> <p>Log periodically:</p> <ul> <li><strong>Main/DLQ sizes</strong>, <strong>processing rates</strong>, <strong>retry counts</strong></li> <li><strong>Graveyard size</strong></li> </ul> <p>Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--- Accumulated Benchmark Metrics ---
Elapsed Time: 10.00 s
Total Successful Requests: 834
Total Failed Requests: 0
Total Throughput: 83.38 req/s
Average Latency: 322.23 ms
Queue Sizes - Main: 40, DLQ: 0, Graveyard: 0
Average Queue Sizes - Main: 22.00, DLQ: 0.00
</code></pre></div></div> <hr/> <h2 id="addressing-queue-bloating">Addressing Queue Bloating</h2> <p>Root cause: <strong>generation rate &gt; consumption rate</strong> with only <strong>5 keys</strong> (5 consumers). Add <strong>backpressure</strong> to <code class="language-plaintext highlighter-rouge">generate_requests()</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">queue</span><span class="p">.</span><span class="nf">qsize</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">max_queue_size</span><span class="p">:</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">continue</span>
</code></pre></div></div> <p>Or increase consumer throughput (more keys/parallelism) or <strong>multithreading</strong> consumers.</p> <hr/> <h2 id="exploring-multithreading">Exploring Multithreading</h2> <h3 id="rationale">Rationale</h3> <p>Multiple threads dequeue concurrently → <strong>fewer TTL expirations</strong>.</p> <blockquote> <h5 id="warning-2">WARNING</h5> <p class="block-warning">Python’s <strong>GIL</strong> limits CPU-parallelism. Threads help I/O, but add <strong>contention</strong> and <strong>context switching</strong>.</p> </blockquote> <h3 id="changes-to-the-current-code">Changes to the Current Code</h3> <ol> <li> <p><strong>Threaded Roles</strong></p> <ul> <li>Request generator</li> <li>Exchange-facing workers (per API key)</li> <li>Queue manager (DLQ)</li> <li>Metrics/benchmark printer</li> </ul> </li> <li> <p><strong>Locks on Shared State</strong></p> <ul> <li>Queue manager internals (DLQ, graveyard)</li> <li>If sharing a limiter across threads, make it thread-safe</li> </ul> </li> <li> <p><strong>Nonce Uniqueness</strong></p> <ul> <li>Timestamp <strong>+</strong> thread-local counter or UUID</li> </ul> </li> </ol> <hr/> <h2 id="comparison-between-asynchronous-and-multithreading-client">Comparison Between Asynchronous and Multithreading Client</h2> <h3 id="baseline-comparison">Baseline Comparison</h3> <p><strong>Async</strong></p> <ul> <li>5 key-workers (coroutines)</li> <li>1 generator</li> <li>1 DLQ manager</li> <li>2 monitoring/benchmark coroutines</li> </ul> <p><strong>Threads</strong></p> <ul> <li>5 key-workers (threads)</li> <li>1 generator</li> <li>1 DLQ manager</li> <li>2 monitoring/benchmark threads</li> </ul> <h3 id="observation">Observation</h3> <p><strong>Asynchronous (~84 TPS)</strong></p> <ul> <li>Higher throughput, some queue buildup → <strong>TTL expirations</strong> (e.g., 13) under sustained overload.</li> </ul> <p><strong>Multithreading (~77 TPS)</strong></p> <ul> <li>Slightly lower TPS; keeps queue near <strong>empty</strong> and <strong>avoids expirations</strong>.</li> </ul> <p><strong>CPU Utilization</strong></p> <ul> <li>Async generally lighter; Threads contend on the <strong>GIL</strong>.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li><strong>Async</strong>: Best <strong>TPS / scalability</strong> for I/O; add <strong>backpressure</strong> to avoid TTL lapses.</li> <li><strong>Threads</strong>: Best <strong>delivery reliability</strong> when avoiding expirations is critical.</li> </ul> <hr/> <h2 id="overview-and-modifications-summary">Overview and Modifications Summary</h2> <h3 id="modifying-the-original-client">Modifying the Original Client</h3> <ol> <li><strong>Removed redundant waits</strong> — rely on <strong>circular buffer</strong> limiter</li> <li><strong>Added adaptive buffering</strong> — avoid server-side 429s from timestamp skew</li> <li><strong>Introduced Queue Manager + DLQ + Graveyard</strong> — resilient retries</li> <li><strong>Added backpressure</strong> — keep <strong>qsize</strong> bounded</li> <li><strong>Optional threading</strong> — increase dequeue rate when keys are limited</li> </ol> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>Threading yields <strong>steady delivery</strong> with low queue sizes; Async delivers <strong>higher throughput</strong> with fewer resources. With <strong>adaptive rate control</strong> and <strong>backpressure</strong>, the <strong>async client</strong> becomes the <strong>long-term winner</strong> for speed, efficiency, and scale.</p> <p><a href="https://github.com/vynious/optimise-throughput" target="_blank" class="btn btn-primary">View Repository on GitHub</a></p>]]></content><author><name></name></author><category term="engineering"/><category term="performance"/><category term="python"/><category term="concurrency"/><category term="asyncio"/><category term="threading"/><category term="rate-limiting"/><category term="kafka"/><category term="queues"/><category term="benchmarking"/><summary type="html"><![CDATA[Review and optimization of a high-throughput Python client under server rate limits — benchmarking, adaptive buffering, DLQ, and async vs threading trade-offs.]]></summary></entry></feed>