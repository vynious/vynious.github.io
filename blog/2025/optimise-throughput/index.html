<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maximizing Client Throughput — Async vs Threads, Adaptive Rate Limiting, and Queue Resilience | Shawn Thiah </title> <meta name="author" content="Shawn Thiah"> <meta name="description" content="Review and optimization of a high-throughput Python client under server rate limits — benchmarking, adaptive buffering, DLQ, and async vs threading trade-offs."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%A3&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vynious.github.io/blog/2025/optimise-throughput/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shawn</span> Thiah </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximizing Client Throughput — Async vs Threads, Adaptive Rate Limiting, and Queue Resilience</h1> <p class="post-meta"> Created on September 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> performance</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/concurrency"> <i class="fa-solid fa-hashtag fa-sm"></i> concurrency</a>   <a href="/blog/tag/asyncio"> <i class="fa-solid fa-hashtag fa-sm"></i> asyncio</a>   <a href="/blog/tag/threading"> <i class="fa-solid fa-hashtag fa-sm"></i> threading</a>   <a href="/blog/tag/rate-limiting"> <i class="fa-solid fa-hashtag fa-sm"></i> rate-limiting</a>   <a href="/blog/tag/kafka"> <i class="fa-solid fa-hashtag fa-sm"></i> kafka</a>   <a href="/blog/tag/queues"> <i class="fa-solid fa-hashtag fa-sm"></i> queues</a>   <a href="/blog/tag/benchmarking"> <i class="fa-solid fa-hashtag fa-sm"></i> benchmarking</a>   ·   <a href="/blog/category/engineering"> <i class="fa-solid fa-tag fa-sm"></i> engineering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>Prefer a <strong>single authoritative limiter</strong> (circular buffer) and remove redundant sleeps. Add a <strong>small adaptive buffer</strong> for server-side timestamp skew to avoid 429s.</p> </blockquote> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>If <strong>request generation &gt; dequeue rate</strong>, your main queue will bloat and TTLs will expire. Add <strong>backpressure</strong> or increase <strong>consumer parallelism</strong>.</p> </blockquote> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>Relying on a <strong>fixed latency buffer</strong> (e.g., always 50 ms) can silently cut your throughput on good networks <strong>or</strong> still 429 on bad ones. Use <strong>adaptive buffering</strong>.</p> </blockquote> <hr> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><strong><a href="#getting-started">Getting Started</a></strong></li> <li><strong><a href="#folder-structure">Folder Structure</a></strong></li> <li><strong><a href="#introduction">Introduction</a></strong></li> <li> <strong><a href="#benchmarking-system">Benchmarking System</a></strong> <ul> <li><a href="#key-metrics-tracked">Key Metrics Tracked</a></li> <li><a href="#how-the-benchmarking-system-works">How the Benchmarking System Works</a></li> </ul> </li> <li> <strong><a href="#enhancing-the-rate-limiter">Enhancing the Rate Limiter</a></strong> <ul> <li><a href="#original-implementation">Original Implementation</a></li> <li><a href="#identified-issues">Identified Issues</a></li> <li><a href="#proposed-solution">Proposed Solution</a></li> <li><a href="#observation-performance-improvement">Observation: Performance Improvement</a></li> <li><a href="#potential-issue-429-errors-due-to-latency">Potential Issue: 429 Errors Due to Latency</a></li> <li><a href="#improved-version-adaptive-buffering">Improved Version: Adaptive Buffering</a></li> </ul> </li> <li> <strong><a href="#improving-the-queue-system">Improving the Queue System</a></strong> <ul> <li><a href="#issue">Issue</a></li> <li><a href="#solution-queue-manager-with-dead-letter-queue-dlq">Solution: Queue Manager with Dead Letter Queue (DLQ)</a></li> <li><a href="#lifecycle-with-queue-manager">Lifecycle with Queue Manager</a></li> <li><a href="#monitoring-the-queue">Monitoring the Queue</a></li> </ul> </li> <li><strong><a href="#addressing-queue-bloating">Addressing Queue Bloating</a></strong></li> <li> <strong><a href="#exploring-multithreading">Exploring Multithreading</a></strong> <ul> <li><a href="#rationale">Rationale</a></li> <li><a href="#changes-to-the-current-code">Changes to the Current Code</a></li> </ul> </li> <li> <strong><a href="#comparison-between-asynchronous-and-multithreading-client">Comparison Between Asynchronous and Multithreading Client</a></strong> <ul> <li><a href="#baseline-comparison">Baseline Comparison</a></li> <li><a href="#observation">Observation</a></li> <li><a href="#summary">Summary</a></li> </ul> </li> <li> <strong><a href="#overview-and-modifications-summary">Overview and Modifications Summary</a></strong> <ul> <li><a href="#modifying-the-original-client">Modifying the Original Client</a></li> </ul> </li> <li><strong><a href="#final-thoughts">Final Thoughts</a></strong></li> </ol> <hr> <h2 id="getting-started">Getting Started</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># run the server</span>
python3 original_server.py

<span class="c"># run async client</span>
python3 async/client.py

<span class="c"># run multithreaded client</span>
python3 thread/client.py

<span class="c"># run with memory profiling</span>
mprof run python3 &lt;MODEL&gt;/client.py
mprof plot
</code></pre></div></div> <hr> <h2 id="folder-structure">Folder Structure</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">async/</code> — asynchronous client (rate limiter, benchmark, queue manager)</li> <li> <code class="language-plaintext highlighter-rouge">thread/</code> — multithreaded client (thread-safe queue manager and limiter)</li> <li> <code class="language-plaintext highlighter-rouge">original_client.py</code> — baseline client</li> <li> <code class="language-plaintext highlighter-rouge">original_server.py</code> — baseline server</li> </ul> <hr> <h2 id="introduction">Introduction</h2> <p>This post reviews and optimizes a Python client designed to <strong>maximize throughput</strong> under <strong>server-enforced rate limits</strong> using multiple API keys. We identify bottlenecks in the original implementation, introduce an <strong>adaptive rate limiter</strong>, add <strong>queue resilience</strong> with a <strong>DLQ</strong>, and compare <strong>async vs threading</strong> under realistic load.</p> <hr> <h2 id="benchmarking-system">Benchmarking System</h2> <blockquote> <h5 id="tip-1">TIP</h5> <p class="block-tip">Benchmarks live in <code class="language-plaintext highlighter-rouge">async/benchmark.py</code> and <code class="language-plaintext highlighter-rouge">thread/benchmark.py</code>. Keep runner overhead minimal and print <strong>aggregate stats</strong> periodically.</p> </blockquote> <h3 id="key-metrics-tracked">Key Metrics Tracked</h3> <ol> <li> <strong>Total Successful Requests</strong> — stability and efficiency under load</li> <li> <strong>Total Failed Requests</strong> — timeouts / 429s / network issues</li> <li> <strong>Average Latency (ms)</strong> — user-perceived performance</li> <li> <strong>Throughput (TPS)</strong> — sustainable capacity from start of run</li> </ol> <h3 id="how-the-benchmarking-system-works">How the Benchmarking System Works</h3> <ul> <li>Records success/failure and latencies</li> <li>Computes moving <strong>average latency</strong> </li> <li>Derives <strong>throughput</strong> since benchmark start</li> <li>Prints metrics at fixed intervals for <strong>real-time feedback</strong> </li> </ul> <hr> <h2 id="enhancing-the-rate-limiter">Enhancing the Rate Limiter</h2> <h3 id="original-implementation">Original Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RateLimiter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">per_second_rate</span><span class="p">,</span> <span class="n">min_duration_ms_between_requests</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span> <span class="o">=</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span> <span class="o">=</span> <span class="n">min_duration_ms_between_requests</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@contextlib.asynccontextmanager</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">acquire</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timeout_ms</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">enter_ms</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">now</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">enter_ms</span> <span class="o">&gt;</span> <span class="n">timeout_ms</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">RateLimiterTimeout</span><span class="p">()</span>

            <span class="c1"># Fixed Interval Check
</span>            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Circular Buffer Check
</span>            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="k">break</span>

        <span class="n">self</span><span class="p">.</span><span class="n">__last_request_time</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">now</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span>
        <span class="k">yield</span> <span class="n">self</span>
</code></pre></div></div> <h3 id="identified-issues">Identified Issues</h3> <p>Two separate sleeps attempt to regulate rate:</p> <ul> <li> <strong>Fixed interval</strong> between consecutive requests</li> <li> <strong>Circular buffer</strong> enforcing N/second</li> </ul> <p>They are <strong>redundant</strong> and cause extra context switches.</p> <h3 id="proposed-solution">Proposed Solution</h3> <p>Remove the <strong>fixed interval</strong>; rely solely on the <strong>circular buffer</strong> (N requests per any sliding 1 s window). This reduces yield/scheduling overhead and still handles <strong>bursty</strong> and <strong>constant-rate</strong> traffic.</p> <h3 id="observation-performance-improvement">Observation: Performance Improvement</h3> <p>Throughput improved from <strong>~74 TPS → ~85 TPS</strong> after removing the fixed interval check, primarily by cutting <strong>event-loop churn</strong>.</p> <h3 id="potential-issue-429-errors-due-to-latency">Potential Issue: 429 Errors Due to Latency</h3> <p>Even with a correct client window, <strong>server-side timestamps</strong> (affected by variable network latency) can observe <strong>Δt &lt; 1000 ms</strong> between the 1st and Nth requests → <strong>429</strong>.</p> <blockquote> <h5 id="warning-1">WARNING</h5> <p class="block-warning">Timestamp skew of just a few milliseconds between client/server can flip a pass into a fail. Add a <strong>latency headroom</strong>.</p> </blockquote> <h3 id="improved-version-adaptive-buffering">Improved Version: Adaptive Buffering</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RateLimiter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">per_second_rate</span><span class="p">,</span> <span class="n">min_duration_ms_between_requests</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span> <span class="o">=</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__min_duration_ms_between_requests</span> <span class="o">=</span> <span class="n">min_duration_ms_between_requests</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">per_second_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span> <span class="o">=</span> <span class="mi">40</span>     <span class="c1"># ms
</span>        <span class="n">self</span><span class="p">.</span><span class="n">__min_buffer</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># ms
</span>        <span class="n">self</span><span class="p">.</span><span class="n">__max_buffer</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># ms
</span>
    <span class="k">def</span> <span class="nf">update_buffer</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">:</span>
            <span class="n">avg_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__max_buffer</span><span class="p">,</span>
                                <span class="nf">max</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__min_buffer</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">avg_latency</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">record_latency</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latency_ms</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__latency_window</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency_ms</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">update_buffer</span><span class="p">()</span>

    <span class="nd">@contextlib.asynccontextmanager</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">acquire</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timeout_ms</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">enter_ms</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="c1"># headroom: circular window (1000ms) + adaptive buffer
</span>        <span class="n">headroom</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">__buffer</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">now</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">enter_ms</span> <span class="o">&gt;</span> <span class="n">timeout_ms</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">RateLimiterTimeout</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">headroom</span><span class="p">:</span>
                <span class="n">sleep_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">headroom</span> <span class="o">-</span> <span class="p">(</span><span class="n">now</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">1000</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">break</span>

        <span class="n">self</span><span class="p">.</span><span class="n">__request_times</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span><span class="p">]</span> <span class="o">=</span> <span class="nf">timestamp_ms</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__curr_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">__per_second_rate</span>
        <span class="k">yield</span> <span class="n">self</span>
</code></pre></div></div> <blockquote> <h5 id="tip-2">TIP</h5> <p class="block-tip">Still seeing sporadic 429s? Add <strong>exponential backoff with jitter</strong> on retries and clamp max in-flight requests per key.</p> </blockquote> <hr> <h2 id="improving-the-queue-system">Improving the Queue System</h2> <h3 id="issue">Issue</h3> <p>Request generation can <strong>outpace</strong> consumption: TTLs expire, wasting work.</p> <h3 id="solution-queue-manager-with-dead-letter-queue-dlq">Solution: Queue Manager with Dead Letter Queue (DLQ)</h3> <ul> <li> <strong>Main Queue</strong> — normal flow</li> <li> <strong>DLQ</strong> — failures/timeouts for <strong>retry</strong> with capped attempts</li> <li> <strong>Graveyard</strong> — IDs exceeding max retries for later analysis</li> </ul> <p><strong>Retry Prioritization:</strong> Insert short <strong>cooldowns</strong> in producers so DLQ items can be re-slotted quickly (lightweight alternative to a strict priority queue).</p> <h3 id="lifecycle-with-queue-manager">Lifecycle with Queue Manager</h3> <p><img src="./img/uml_seq_diagram.png" alt="New Sequence with Queue Manager"></p> <h3 id="monitoring-the-queue">Monitoring the Queue</h3> <p>Log periodically:</p> <ul> <li> <strong>Main/DLQ sizes</strong>, <strong>processing rates</strong>, <strong>retry counts</strong> </li> <li><strong>Graveyard size</strong></li> </ul> <p>Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--- Accumulated Benchmark Metrics ---
Elapsed Time: 10.00 s
Total Successful Requests: 834
Total Failed Requests: 0
Total Throughput: 83.38 req/s
Average Latency: 322.23 ms
Queue Sizes - Main: 40, DLQ: 0, Graveyard: 0
Average Queue Sizes - Main: 22.00, DLQ: 0.00
</code></pre></div></div> <hr> <h2 id="addressing-queue-bloating">Addressing Queue Bloating</h2> <p>Root cause: <strong>generation rate &gt; consumption rate</strong> with only <strong>5 keys</strong> (5 consumers). Add <strong>backpressure</strong> to <code class="language-plaintext highlighter-rouge">generate_requests()</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">queue</span><span class="p">.</span><span class="nf">qsize</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">max_queue_size</span><span class="p">:</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">continue</span>
</code></pre></div></div> <p>Or increase consumer throughput (more keys/parallelism) or <strong>multithreading</strong> consumers.</p> <hr> <h2 id="exploring-multithreading">Exploring Multithreading</h2> <h3 id="rationale">Rationale</h3> <p>Multiple threads dequeue concurrently → <strong>fewer TTL expirations</strong>.</p> <blockquote> <h5 id="warning-2">WARNING</h5> <p class="block-warning">Python’s <strong>GIL</strong> limits CPU-parallelism. Threads help I/O, but add <strong>contention</strong> and <strong>context switching</strong>.</p> </blockquote> <h3 id="changes-to-the-current-code">Changes to the Current Code</h3> <ol> <li> <p><strong>Threaded Roles</strong></p> <ul> <li>Request generator</li> <li>Exchange-facing workers (per API key)</li> <li>Queue manager (DLQ)</li> <li>Metrics/benchmark printer</li> </ul> </li> <li> <p><strong>Locks on Shared State</strong></p> <ul> <li>Queue manager internals (DLQ, graveyard)</li> <li>If sharing a limiter across threads, make it thread-safe</li> </ul> </li> <li> <p><strong>Nonce Uniqueness</strong></p> <ul> <li>Timestamp <strong>+</strong> thread-local counter or UUID</li> </ul> </li> </ol> <hr> <h2 id="comparison-between-asynchronous-and-multithreading-client">Comparison Between Asynchronous and Multithreading Client</h2> <h3 id="baseline-comparison">Baseline Comparison</h3> <p><strong>Async</strong></p> <ul> <li>5 key-workers (coroutines)</li> <li>1 generator</li> <li>1 DLQ manager</li> <li>2 monitoring/benchmark coroutines</li> </ul> <p><strong>Threads</strong></p> <ul> <li>5 key-workers (threads)</li> <li>1 generator</li> <li>1 DLQ manager</li> <li>2 monitoring/benchmark threads</li> </ul> <h3 id="observation">Observation</h3> <p><strong>Asynchronous (~84 TPS)</strong></p> <ul> <li>Higher throughput, some queue buildup → <strong>TTL expirations</strong> (e.g., 13) under sustained overload.</li> </ul> <p><strong>Multithreading (~77 TPS)</strong></p> <ul> <li>Slightly lower TPS; keeps queue near <strong>empty</strong> and <strong>avoids expirations</strong>.</li> </ul> <p><strong>CPU Utilization</strong></p> <ul> <li>Async generally lighter; Threads contend on the <strong>GIL</strong>.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li> <strong>Async</strong>: Best <strong>TPS / scalability</strong> for I/O; add <strong>backpressure</strong> to avoid TTL lapses.</li> <li> <strong>Threads</strong>: Best <strong>delivery reliability</strong> when avoiding expirations is critical.</li> </ul> <hr> <h2 id="overview-and-modifications-summary">Overview and Modifications Summary</h2> <h3 id="modifying-the-original-client">Modifying the Original Client</h3> <ol> <li> <strong>Removed redundant waits</strong> — rely on <strong>circular buffer</strong> limiter</li> <li> <strong>Added adaptive buffering</strong> — avoid server-side 429s from timestamp skew</li> <li> <strong>Introduced Queue Manager + DLQ + Graveyard</strong> — resilient retries</li> <li> <strong>Added backpressure</strong> — keep <strong>qsize</strong> bounded</li> <li> <strong>Optional threading</strong> — increase dequeue rate when keys are limited</li> </ol> <hr> <h2 id="final-thoughts">Final Thoughts</h2> <p>Threading yields <strong>steady delivery</strong> with low queue sizes; Async delivers <strong>higher throughput</strong> with fewer resources. With <strong>adaptive rate control</strong> and <strong>backpressure</strong>, the <strong>async client</strong> becomes the <strong>long-term winner</strong> for speed, efficiency, and scale.</p> <p><a href="https://github.com/vynious/optimise-throughput" target="_blank" class="btn btn-primary" rel="external nofollow noopener">View Repository on GitHub</a></p> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>